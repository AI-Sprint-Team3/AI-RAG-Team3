import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from typing import List, Dict
from src.generation.prompt_selector import select_prompt_auto, select_prompt_manual, select_prompt_by_intent
from src.generation.response_postprocess import clean_response
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document  
from src.generation.utils import cross_check_answer, extract_keywords_from_text

class LLMPipeline:
  """
  RAG + ë©€í‹°í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì› LLM íŒŒì´í”„ë¼ì¸
  """
  
  def __init__(self, llm, retriever=None):
    """
      llm(OpenAILLM): OpenAI ê¸°ë°˜ LLM ë˜í¼
      retriever (Optional): ë²¡í„°DB retriever
    """
    self.llm = llm
    self.retriever = retriever
    # self.chat_history: List[Dict] = []  # {"role": "user"/"assistant", "content": str}
    self.memory = ConversationBufferMemory(return_messages=True)  # âœ… ì¶”ê°€
    
  def run(self, query: str, contexts:List[Document] = None, prompt_type: str = None) -> str:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°›ì•„ LLM ì‘ë‹µ ìƒì„±
    - prompt_typeì´ ì£¼ì–´ì§€ë©´ ìˆ˜ë™ ì„ íƒ
    - ì—†ìœ¼ë©´ ìë™ ë¶„ê¸°(select_prompt)
    """
    
    # 1) RAG context ê°€ì ¸ì˜¤ê¸°
    context = ""
    if contexts is None and self.retriever:
      results = self.retriever(query)   # advanced_retrieve ë°˜í™˜ê°’
      context_docs = self.convert_to_documents(results)
      context = "\n\n".join([doc.page_content for doc in context_docs])
    else:
      context = contexts or ""
      
    # 2-1) í”„ë¡¬í”„íŠ¸ ì„ íƒ - í‚¤ì›Œë“œ
    # if prompt_type:
    #   prompt_template = select_prompt_manual(prompt_type)
    # else:
    #   prompt_template = select_prompt_auto(query)
    
    # 2-2) í”„ë¡¬í”„íŠ¸ ì„ íƒ - llm
    prompt_template = select_prompt_by_intent(query)
    
    # 3) memory ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì „ ëŒ€í™” ê¸°ë¡)
    history = self.memory.load_memory_variables({})["history"]

    
    # 4) LLM í˜¸ì¶œ
    raw_response = self.llm.generate(
      question=query,             # ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸
      template=prompt_template,   # ì„ íƒëœ í”„ë¡¬í”„íŠ¸
      rag_context=context,         # RAG ê²€ìƒ‰ ê²°ê³¼
      history=history
    )
    
    # 5) í›„ì²˜ë¦¬
    response = clean_response(raw_response)
    
    # 6) íˆìŠ¤í† ë¦¬ ì¶”ê°€
    self.memory.save_context(
      {"input": query},   # user message
      {"output": response}  # assistant message
      )
    
    # 7) cross-check ì ìš©
    candidates = [{"content": doc.page_content, "meta": doc.metadata} for doc in context]
    cross_result = cross_check_answer(response, candidates)
    
    # 8) ìµœì¢…ë°˜í™”: LLM ì‘ë‹µ + ê·¼ê±° ê²€ì¦ ê²°ê³¼
    return {
      "answer": response,
      "cross_check": cross_result
    }

  def convert_to_documents(self, results: List[Dict], include_score: bool = True) -> List[Document]:
    """
    advanced_retrieve ë“± ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    docs = []
    for r in results:
      meta = r["meta"].copy()
      if include_score:
        meta["score"] = r.get("score", 0)
        
      docs.append(Document(page_content=r["text"], metadata=meta))
    return docs
  

  @property
  def chat_history(self):
    # llm_openai.OpenAIRAGClient ì•ˆì— ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬í„´
    return self.llm.conversation_history


# -----------------------
# retriever ë˜í•‘ ì˜ˆì œ
# -----------------------
def make_retriever(collection, embedding_fn, top_k=3, use_mmr=True, use_bm25=True, bm25=None, filter_title=None):
  """
  advanced_retrieve ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë°˜í™˜
  - use_mmr=True: MMR ê¸°ë°˜ ê²€ìƒ‰
  - filter_title: ì œëª©(metadata) í•„í„°ë§
  """
  from src.retrieval.retriever import advanced_retrieve

  def retriever_fn(query):
    print(f"=== advanced_retrieve í˜¸ì¶œ ===")
    print(f"filter_title: {filter_title}")
    results = advanced_retrieve(
      query=query,
      collection=collection,
      embedding_fn=embedding_fn,
      top_k=top_k,
      use_mmr=use_mmr,
      use_bm25=use_bm25,
      bm25=bm25,
      agency_filter=filter_title,
    )
    return results
  
  return retriever_fn

def run_queries(pipeline, queries: List[str]):
  """
  ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ pipelineì— ëŒë ¤ì„œ ê²°ê³¼ í™•ì¸
  """
  for q in queries:
    results = pipeline.retriever(q)  # retrieverì— í•„í„°/ì˜µì…˜ ì ìš©ë¨
    contexts = pipeline.convert_to_documents(results, include_score=True)
    response = pipeline.run(q, contexts)
    # response = pipeline.run(q)
    cross_check = response['cross_check']
    
    print(f"Q: {q}")
    print(f"A: {response['answer']}\n")
    print(f"Confidence: {cross_check['confidence']:.2f} | Validity: {cross_check['validity']}\n")
    
    print("Matched keywords:", ", ".join(cross_check['matched_keywords']) or "None")
    ak = cross_check['answer_keywords']
    print("Answer keywords:", ", ".join(ak[:10]) + (", ..." if len(ak) > 10 else ""))
    ck = cross_check['candidate_keywords']
    print("Candidate keywords:", ", ".join(ck[:10]) + (", ..." if len(ck) > 10 else ""))
    
    print("\n" + "*" * 50 + "\n")

if __name__ == "__main__":
  from functools import partial
  from src.embeddings.embedder import EmbedderFactory
  from src.embeddings.vectorstore_chroma import get_collection, add_docs_to_chroma
  from src.retrieval.retriever import advanced_retrieve
  from src.retrieval.bm25_helper import BM25Helper
  from src.generation.llm_openai import OpenAIRAGClient
  from config.settings import COLLECTION_NAME
  from src.pipelines.query_pipeline import load_docs
  
  
  # 1) LLM ê°ì²´ ì¤€ë¹„ (OpenAILLMClient)
  llm = OpenAIRAGClient(temperature=0.7)
  
  # 2) ë²¡í„° DB & ì„ë² ë”© ì£¼ë¹„
  collection = get_collection(COLLECTION_NAME)
  embedding_fn = EmbedderFactory.get_embedder(provider="openai")

  # ë¬¸ì„œ ì—…ë¡œë“œ (ë°°ì¹˜)
  documents = load_docs()
  # print("ğŸ“Œ ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘...")
  # add_docs_to_chroma(documents, collection=collection, embedding_fn=embedding_fn, batch_size=8)
  # print("âœ… ì—…ë¡œë“œ ì™„ë£Œ")
      
  # 3) BM25 ì¤€ë¹„
  # print(f"âœ… ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ ìˆ˜: {len(documents)}")
  corpus_texts = [d.get("texts", {}).get("merged", "") for d in documents if d.get("texts", {}).get("merged", "")]
  bm25_helper = BM25Helper(corpus_texts)
  
  # 4) advanced_retrieve ë˜í•‘
  retriever = make_retriever(
    collection=collection,
    embedding_fn=embedding_fn,
    bm25=bm25_helper,
    top_k=3,
    use_mmr=True,
    filter_title='ì „ë¶íŠ¹ë³„ìì¹˜ë„_ì •ìì‹œ_ì •ìì²´ìœ¡íŠ¸ë ˆì´ë‹ì„¼í„°_í†µí•©ìš´ì˜ê´€ë¦¬ì‹œìŠ¤í…œ_êµ¬'
  )
  
  # 5) LLMPipeLine ìƒì„±
  pipeline = LLMPipeline(llm=llm, retriever=retriever)
  
  # 6) í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
  # ì¿¼ë¦¬ ì‹¤í–‰ - ì—¬ëŸ¬ê°œ
  
  queries = [
      "ì´ ì‚¬ì—…ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
      "ê³¼ì—… ìˆ˜í–‰ ê¸°ê°„ì€ ì–¸ì œë¶€í„° ì–¸ì œê¹Œì§€ì¸ê°€ìš”?"
  ]

  run_queries(pipeline, queries)
