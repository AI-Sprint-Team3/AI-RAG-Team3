import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from typing import List, Dict
from src.generation.prompt_selector import select_prompt_auto, select_prompt_manual, select_prompt_by_intent
from src.generation.response_postprocess import pretty_format_answer
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document  
from src.generation.utils import cross_check_answer, extract_keywords_from_text

class LLMPipeline:
  """
  RAG + ë©€í‹°í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì§€ì› LLM íŒŒì´í”„ë¼ì¸
  """
  
  def __init__(self, llm, retriever=None, confidence_threshold: float = 0.5, max_retry: int = 2):
    """
      llm(OpenAILLM): OpenAI ê¸°ë°˜ LLM ë˜í¼
      retriever (Optional): ë²¡í„°DB retriever
    """
    self.llm = llm
    self.retriever = retriever
    # self.chat_history: List[Dict] = []  # {"role": "user"/"assistant", "content": str}
    self.memory = ConversationBufferMemory(return_messages=True)  # âœ… ì¶”ê°€
    self.confidence_threshold = confidence_threshold  # Confidence ê¸°ì¤€
    self.max_retry = max_retry  # ì¬ê²€ìƒ‰ ìµœëŒ€ íšŸìˆ˜
    
    
  def run(self, query: str, contexts:List[Document] = None, prompt_type: str = None) -> str:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë°›ì•„ LLM ì‘ë‹µ ìƒì„±
    - prompt_typeì´ ì£¼ì–´ì§€ë©´ ìˆ˜ë™ ì„ íƒ
    - ì—†ìœ¼ë©´ ìë™ ë¶„ê¸°(select_prompt)
    """
    
    # 1) RAG context ê°€ì ¸ì˜¤ê¸°
    context = ""
    if contexts is None and self.retriever:
      results = self.retriever(query)   # advanced_retrieve ë°˜í™˜ê°’
      context_docs = self.convert_to_documents(results)
      context = "\n\n".join([doc.page_content for doc in context_docs])
    else:
      context = contexts or ""
      
    # 2-1) í”„ë¡¬í”„íŠ¸ ì„ íƒ - í‚¤ì›Œë“œ
    # if prompt_type:
    #   prompt_template = select_prompt_manual(prompt_type)
    # else:
    #   prompt_template = select_prompt_auto(query)
    
    # 2-2) í”„ë¡¬í”„íŠ¸ ì„ íƒ - llm
    prompt_template = select_prompt_by_intent(query)
    
    # 3) memory ë¶ˆëŸ¬ì˜¤ê¸° (ì´ì „ ëŒ€í™” ê¸°ë¡)
    history = self.memory.load_memory_variables({})["history"]

    
    # 4) LLM í˜¸ì¶œ
    raw_response = self.llm.generate(
      question=query,             # ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸
      template=prompt_template,   # ì„ íƒëœ í”„ë¡¬í”„íŠ¸
      rag_context=context,         # RAG ê²€ìƒ‰ ê²°ê³¼
      history=history
    )
    
    # 5) í›„ì²˜ë¦¬
    response = pretty_format_answer(raw_response)
    
    # 6) íˆìŠ¤í† ë¦¬ ì¶”ê°€
    self.memory.save_context(
      {"input": query},   # user message
      {"output": response}  # assistant message
      )
    
    # 7) cross-check ì ìš©
    candidates = [{"content": doc.page_content, "meta": doc.metadata} for doc in context]
    cross_result = cross_check_answer(response, candidates)
    
    # 8) ìµœì¢…ë°˜í™”: LLM ì‘ë‹µ + ê·¼ê±° ê²€ì¦ ê²°ê³¼
    return {
      "answer": response,
      "cross_check": cross_result
    }
  
  def run_with_confidence_loop(self, query:str, contexts: List[Document] = None, prompt_type: str = None):
    """
    Confidence ì„ê³„ê°’ ê¸°ë°˜ ì¬ê²€ìƒ‰ + ë‹µë³€ ì¬ìƒì„± ë£¨í”„
    - query: ì‚¬ìš©ì ì§ˆë¬¸
    - contexts: ê²€ìƒ‰ ê²°ê³¼ Document ë¦¬ìŠ¤íŠ¸
    """
    attempts = 0
    response_data = None
    
    # self.retriever_originalì— ì›ë˜ retriever í•¨ìˆ˜ ì €ì¥
    if not hasattr(self, "retriever_original"):
      self.retriever_original = self.retriever
    while attempts <= self.max_retry:
      # run() í˜¸ì¶œ
      if not contexts or len(contexts) == 0:
        contexts = self.retriever_original(query)  # advanced_retrieve ë˜í•‘
        if not contexts:
          print("âš ï¸ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. LLM í˜¸ì¶œ ì—†ì´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
          return None   # ì¢…ë£Œ
        
      response = self.run(query, contexts, prompt_type)
      confidence = response['cross_check'].get("confidence", 0)
      
      # confidence í™•ì¸
      if confidence >= self.confidence_threshold:
        response_data = response
        break
      else:
        print(f"âš ï¸ Confidence ë‚®ìŒ ({confidence:.2f}), ì¬ê²€ìƒ‰ ë° ë‹µë³€ ì¬ìƒì„± ì¤‘... (ì‹œë„ {attempts})")
        
        # ì¬ê²€ìƒ‰ ì „ëµ: top_k ì¦ê°€
        top_k_retry = 3 + attempts
        if self.retriever_original:
          results = self.retriever_original(query, top_k=top_k_retry)
          contexts = self.convert_to_documents(results)

        # # contexts ì¬ìƒì„±
        if not contexts or len(contexts) == 0:
          results = self.retriever_original(query, top_k=top_k_retry)
          contexts = self.convert_to_documents(results)

        attempts += 1
        
    if response_data is None:
      response_data = response  # ë§ˆì§€ë§‰ ì‹œë„ ê²°ê³¼ ë°˜í™˜
    
    return response_data
    

  def convert_to_documents(self, results: List[Dict], include_score: bool = True) -> List[Document]:
    """
    advanced_retrieve ë“± ê²€ìƒ‰ ê²°ê³¼ë¥¼ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    """
    docs = []
    for r in results:
      meta = r["meta"].copy()
      if include_score:
        meta["score"] = r.get("score", 0)
        
      docs.append(Document(page_content=r["text"], metadata=meta))
    return docs
  

  @property
  def chat_history(self):
    # llm_openai.OpenAIRAGClient ì•ˆì— ì €ì¥ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¦¬í„´
    return self.llm.conversation_history


# -----------------------
# retriever ë˜í•‘ ì˜ˆì œ
# -----------------------
def make_retriever(collection, embedding_fn, top_k=3, use_mmr=True, use_bm25=True, bm25=None, filter_title=None):
  """
  advanced_retrieve ë˜í¼ë¥¼ ë§Œë“¤ì–´ ë°˜í™˜
  - use_mmr=True: MMR ê¸°ë°˜ ê²€ìƒ‰
  - filter_title: ì œëª©(metadata) í•„í„°ë§
  """
  from src.retrieval.retriever import advanced_retrieve

  def retriever_fn(query):
    print(f"=== advanced_retrieve í˜¸ì¶œ ===")
    print(f"filter_title: {filter_title}")
    results = advanced_retrieve(
      query=query,
      collection=collection,
      embedding_fn=embedding_fn,
      top_k=top_k,
      use_mmr=use_mmr,
      use_bm25=use_bm25,
      bm25=bm25,
      agency_filter=filter_title,
    )
    return results
  
  return retriever_fn

def run_queries(pipeline, queries: List[str]):
  """
  ì—¬ëŸ¬ ì¿¼ë¦¬ë¥¼ pipelineì— ëŒë ¤ì„œ ê²°ê³¼ í™•ì¸
  """
  for q in queries:
    results = pipeline.retriever(q)  # retrieverì— í•„í„°/ì˜µì…˜ ì ìš©ë¨
    contexts = pipeline.convert_to_documents(results, include_score=True)
    # response = pipeline.run(q, contexts)
    # response = pipeline.run(q)
    
    # confidence_loop ì ìš©
    response = pipeline.run_with_confidence_loop(q, contexts)
    cross_check = response['cross_check']
    
    print(f"Q: {q}")
    print(f"A: {response['answer']}\n")
    print(f"Confidence: {cross_check['confidence']:.2f} | Validity: {cross_check['validity']}\n")
    
    mk = cross_check['matched_keywords']
    print("Matched keywords:", ", ".join(mk[:10]) + (", ..." if len(mk) > 10 else ""))
    ak = cross_check['answer_keywords']
    print("Answer keywords:", ", ".join(ak[:10]) + (", ..." if len(ak) > 10 else ""))
    ck = cross_check['candidate_keywords']
    print("Candidate keywords:", ", ".join(ck[:10]) + (", ..." if len(ck) > 10 else ""))
    
    print("\n" + "*" * 50 + "\n")

if __name__ == "__main__":
  from functools import partial
  from src.embeddings.embedder import EmbedderFactory
  from src.embeddings.vectorstore_chroma import get_collection, add_docs_to_chroma
  from src.retrieval.retriever import advanced_retrieve
  from src.retrieval.bm25_helper import BM25Helper
  from src.generation.llm_openai import OpenAIRAGClient
  from config.settings import COLLECTION_NAME
  from src.pipelines.query_pipeline import load_docs
  
  
  # 1) LLM ê°ì²´ ì¤€ë¹„ (OpenAILLMClient)
  llm = OpenAIRAGClient(temperature=0.7)
  
  # 2) ë²¡í„° DB & ì„ë² ë”© ì£¼ë¹„
  collection = get_collection(COLLECTION_NAME)
  embedding_fn = EmbedderFactory.get_embedder(provider="openai")

  # ë¬¸ì„œ ì—…ë¡œë“œ (ë°°ì¹˜)
  documents = load_docs()
  # print("ğŸ“Œ ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘...")
  # add_docs_to_chroma(documents, collection=collection, embedding_fn=embedding_fn, batch_size=8)
  # print("âœ… ì—…ë¡œë“œ ì™„ë£Œ")
      
  # 3) BM25 ì¤€ë¹„
  # print(f"âœ… ë¶ˆëŸ¬ì˜¨ ë¬¸ì„œ ìˆ˜: {len(documents)}")
  corpus_texts = [d.get("texts", {}).get("merged", "") for d in documents if d.get("texts", {}).get("merged", "")]
  bm25_helper = BM25Helper(corpus_texts)
  
  # 4) advanced_retrieve ë˜í•‘ - ì¡°ê±´ o
  retriever = make_retriever(
    collection=collection,
    embedding_fn=embedding_fn,
    bm25=bm25_helper,
    top_k=3,
    use_mmr=True,
    filter_title='bioin_á„‹á…´á„…á…­á„€á…µá„€á…µá„‰á…¡á†«á„‹á…¥á†¸_á„Œá…©á†¼á„’á…¡á†¸á„Œá…¥á†¼á„‡á…©á„‰á…µá„‰á…³á„á…¦á†·_á„Œá…¥á†¼á„‡á…©á„€á…ªá†«á„…á…µá„€á…µá„€á…ªá†«_á„€á…µá„‚á…³á†¼á„€á…¢á„‰á…¥á†«_á„‰á…¡á„‹á…¥á†¸_2á„á…¡'
  )

  # ì¡°ê±´ x
  # retriever = make_retriever(
  #   collection=collection,
  #   embedding_fn=embedding_fn,
  #   bm25=bm25_helper,
  #   top_k=3,
  #   use_mmr=True,
  # )
  
  # 5) LLMPipeLine ìƒì„±
  pipeline = LLMPipeline(llm=llm, retriever=retriever)
  
  # 6) í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
  # ì¿¼ë¦¬ ì‹¤í–‰ - ì—¬ëŸ¬ê°œ
  queries = [
      "ì´ ì‚¬ì—…ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
      "ê³¼ì—… ìˆ˜í–‰ ê¸°ê°„ì€ ì–¸ì œë¶€í„° ì–¸ì œê¹Œì§€ì¸ê°€ìš”?",
      "ê³¼ì—…ì˜ ë²”ìœ„ëŠ” ë¬´ì—‡ì„ í¬í•¨í•˜ë‚˜ìš”?",
      "ë°œì£¼ ê¸°ê´€ì€ ì–´ë””ì¸ê°€ìš”?",
      "ì…ì°° ì°¸ì—¬ ë§ˆê°ì¼ì€ ì–¸ì œì¸ê°€ìš”?",
      "ì´ ì‚¬ì—…ì˜ ì£¼ìš” ì‚°ì¶œë¬¼ì€ ë¬´ì—‡ì¸ê°€ìš”?",
      "ê³¼ì—… ëŒ€ìƒ ì§€ì—­ì€ ì–´ë””ì¸ê°€ìš”?",
      "ì‚¬ì—… ê¸ˆì•¡ì€ ì–¼ë§ˆì¸ê°€ìš”?",
      "ë¬¸ì„œ ì´ë¦„ ì•Œë ¤ì¤„ ìˆ˜ ìˆì–´?",
      "ì‚¬ì—… ê¸°ê°„ ì•Œë ¤ì¤˜",
      "ì´ ê³µê³ ëŠ” ì¤‘ì†Œê¸°ì—…ë§Œ ì°¸ì—¬ ê°€ëŠ¥í•œê°€ìš”?", # confidence ì²´í¬
      "ì…ì°° ë°©ì‹ì´ ì œí•œê²½ìŸì¸ê°€ìš”, ì•„ë‹ˆë©´ ì¼ë°˜ê²½ìŸì¸ê°€ìš”?",
      "ì‚¬ì—… ìˆ˜í–‰ ì¥ì†ŒëŠ” ì„œìš¸ì¸ê°€ìš”?",  # hallucination ë°©ì§€
      "ì°¸ê°€ìê²© ì œí•œì‚¬í•­ ì¤‘ í•„ìˆ˜ ë³´ìœ  ë©´í—ˆê°€ ìˆë‚˜ìš”?"
      "ì´ ê³µê³ ì˜ ì´ ì§ì› ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?",
      "ê³„ì•½ ë³´ì¦ ê´€ë ¨ëœ íŠ¹ì´ì‚¬í•­ì´ ìˆë‚˜ìš”?",
      "ì œì•ˆì„œ ë°œí‘œëŠ” ì˜¤í”„ë¼ì¸ë§Œ ê°€ëŠ¥í•œê°€ìš”?",
      "ë¬¸ì„œ ìš”ì•½í•´ì¤˜",    # ìš”ì•½
      
  ]
  # queries = [
    # "ì•„ì‹œì•„ ìœ¡ìƒ ê²½ê¸° ëŒ€íšŒ ìš”êµ¬ì‚¬í•­ì´ ë­”ê°€ìš”?"
  # ]

  run_queries(pipeline, queries)
